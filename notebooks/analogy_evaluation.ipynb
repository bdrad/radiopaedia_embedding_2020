{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analogy Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "main_path = \"F:\\\\BDRAD MiniProject\\\\NLP\"\n",
    "\n",
    "#main directory containing analogy components\n",
    "#these should be individual files that contain analogous a:b pairings on separate lines\n",
    "analogy_main_path = os.path.join(main_path,\"analogies\")\n",
    "\n",
    "#list of file paths to each file of analogy components\n",
    "analogy_file_paths = [os.path.join(root_path,file) for (root_path,sub_dirs,files) in os.walk(analogy_main_path,topdown=True) if files for file in files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def analogy_creator(filepath):\n",
    "    \"\"\"\n",
    "    input is list of filepaths to an analogy component file\n",
    "    analogy files are csv files with a,b on each line where for all a in each line, the relationship to b is of the same/similar type\n",
    "    creator will take all permuations of a,bentries and write to a csv file with the full a:b::c:d relationship as a,b,c,d\n",
    "    \"\"\"\n",
    "    with open(filepath) as file:\n",
    "        reader = csv.reader(file)\n",
    "        data = list(reader)\n",
    "        analogies=list(itertools.combinations(data,2))\n",
    "        #each entry in analogies has format ([a,b],[c,d])\n",
    "        analogies_formatted=list()\n",
    "        for i in analogies:\n",
    "            #entry in j has format [a,b]\n",
    "            format_analogy = [word for j in i for word in j]\n",
    "            #only append if all word are unique (the evaluator later will suppress identical output)\n",
    "            if(len(set(format_analogy)) == len(format_analogy)):\n",
    "                analogies_formatted.append(format_analogy)\n",
    "                \n",
    "    return analogies_formatted\n",
    "\n",
    "def csv_write(row_text,output_file_path,append=False):\n",
    "    if append:\n",
    "        with open(output_file_path,'a',newline='',encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            #note the csv will have a trailing newline which may or may not be a problem\n",
    "            writer.writerows(row_text)  \n",
    "    else:\n",
    "        with open(output_file_path,'w',newline='',encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(row_text) \n",
    "            \n",
    "\n",
    "analogy_output_master_path = os.path.join(main_path,\"med_analogy.csv\")  \n",
    "with open(analogy_output_master_path,'a',newline='',encoding='utf-8') as file:\n",
    "    for filepath in analogy_file_paths:\n",
    "        file_name=os.path.basename(filepath)\n",
    "#         file_name_indiv = file_name.replace(\".csv\",\"_analogy.csv\")\n",
    "#         output_analogy_file_path = os.path.join(main_path,file_name_indiv)\n",
    "#         #create separate files for different analogy categories\n",
    "#         csv_write(analogy_creator(filepath),output_analogy_file_path)\n",
    "        #also create master file\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows([[\"#\"+file_name.replace(\".csv\",\"\")]])\n",
    "        writer.writerows(analogy_creator(filepath))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process word embeddings from text file. \n",
    "\n",
    "Will need to have vectors associated with words and word2id dictionary that maps a word to its id in the array of vectors\n",
    "\n",
    "Will also need to create a set of normalized vectors with suffix \"_n\"\n",
    "\n",
    "See: https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analogy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "import bcolz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class AnalogyEvaluate:\n",
    "    def __init__(self,dimensions,glove_path,analogy_path):\n",
    "        self.glove_path = glove_path\n",
    "        self.analogy_path = analogy_path\n",
    "        self.freq_threshold = 2\n",
    "        self.dimensions = dimensions\n",
    "        self.analogies = self.analogy_generate(analogy_path)\n",
    "        self.file_prefix = \"25custom.50d\"\n",
    "        \n",
    "        self.load_word_vectors()\n",
    "        \n",
    "        with open(\"./maxemerling/counts.pkl\",\"rb\") as file:\n",
    "            self.count_dict=pickle.load(file)\n",
    "        \n",
    "        \n",
    "    def load_word_vectors(self):\n",
    "    \n",
    "        ROOT_DIR = os.path.join(f'{self.glove_path}',f'{self.file_prefix}')\n",
    "        vectors = bcolz.open(ROOT_DIR+\".dat\")[:]\n",
    "        self.word_embedding = vectors\n",
    "        \n",
    "        vectors = bcolz.open(ROOT_DIR+\"_n.dat\")[:]\n",
    "        self.word_embedding_n = vectors\n",
    "        \n",
    "        word2idx = pickle.load(open(ROOT_DIR + \"_idx.pkl\", 'rb'))\n",
    "        self.word2idx = word2idx\n",
    "        #create reverse lookup dictionary\n",
    "        id2word = dict((id, word) for word, id in self.word2idx.items())\n",
    "        self.id2word = id2word\n",
    "    \n",
    "    def most_similar(self,positive,negative,topn=10):\n",
    " \n",
    "        cumulative_vecs = list()\n",
    "        for word in positive: cumulative_vecs.append(self.word_embedding_n[self.word2idx[word]])\n",
    "        for word in negative: cumulative_vecs.append(-1 * self.word_embedding_n[self.word2idx[word]])\n",
    "\n",
    "        cumulative_vector = np.array(cumulative_vecs).sum(axis=0)\n",
    "        cumulative_vector /= np.linalg.norm(cumulative_vector)\n",
    "\n",
    "        #when word vector and cumulative_vector are normalized, cosine similarity reduces to a dot product\n",
    "        cos_sim = np.dot(self.word_embedding_n, cumulative_vector)\n",
    "\n",
    "        best = np.argsort(cos_sim)[::-1][:topn + len(positive) + len(negative)+100]\n",
    "\n",
    "        if self.freq_threshold:\n",
    "            result = [(self.id2word[i], cos_sim[i]) for i in best if (self.count_dict[self.id2word[i]]>=self.freq_threshold and self.id2word[i] not in positive\n",
    "                                                               and self.id2word[i] not in negative)]\n",
    "        else:\n",
    "            result = [(self.id2word[i], cos_sim[i]) for i in best if (self.id2word[i] not in positive and self.id2word[i] not in negative)]\n",
    "        \n",
    "        return result[:topn]\n",
    "    \n",
    "    def analogy_generate(self,filepath):\n",
    "        with open(filepath,\"r\") as file:\n",
    "            reader = csv.reader(file)\n",
    "            data = list(reader)\n",
    "            for i in data:\n",
    "                yield i\n",
    "\n",
    "    def eval_analogy(self):\n",
    "        self.analogies = self.analogy_generate(self.analogy_path)\n",
    "        correct_analogies_subcategory = 0\n",
    "        total_analogies_subcategory = 0\n",
    "        skipped_subcategory = 0\n",
    "        indiv_subcategory_correctness = list()\n",
    "        \n",
    "        sub_category_list = list()\n",
    "        correct_list = list()\n",
    "        total_list = list()\n",
    "        skipped_list = list()\n",
    "        indiv_correctness_list=list()\n",
    "        \n",
    "        for analogy in self.analogies:\n",
    "            if analogy[0].startswith(\"#\"):\n",
    "                sub_category_list.append(analogy[0])\n",
    "                correct_list.append(correct_analogies_subcategory)\n",
    "                total_list.append(total_analogies_subcategory)\n",
    "                skipped_list.append(skipped_subcategory)\n",
    "                indiv_correctness_list.append(indiv_subcategory_correctness)\n",
    "\n",
    "                correct_analogies_subcategory = 0\n",
    "                total_analogies_subcategory = 0\n",
    "                skipped_subcategory = 0\n",
    "                indiv_subcategory_correctness=list()\n",
    "                print(analogy)\n",
    "                pass\n",
    "            else:\n",
    "                positives = [analogy[1],analogy[2]]\n",
    "                negatives = [analogy[0]]\n",
    "                \n",
    "                try:\n",
    "                    most_similar_vec=self.most_similar(positives,negatives,topn=3)\n",
    "\n",
    "                    #give credit if target word is in topn results\n",
    "                    indiv_subcategory_correctness.append(0)\n",
    "                    \n",
    "                    for candidate_word in most_similar_vec:\n",
    "                        #candidate_word is a tuple with the word and its cossim with 3CosAdd vector of input\n",
    "                        print(analogy,\" \",candidate_word)\n",
    "                        if (candidate_word[0] == analogy[3]) :\n",
    "                            #print(\"CORRECT\")\n",
    "                            correct_analogies_subcategory +=1\n",
    "                            indiv_subcategory_correctness[-1]=1\n",
    "                            #can stop printing if correct, saves space. if we remove the break, the counts will still be correct assuming model doesn't give two identical outputs\n",
    "                            break\n",
    "                        \n",
    "                    total_analogies_subcategory+=1\n",
    "\n",
    "                except:\n",
    "                    #will enter this block if word is not found in vocab\n",
    "                    print(analogy,\"Error - skipping\")\n",
    "                    skipped_subcategory +=1\n",
    "                    total_analogies_subcategory+=1\n",
    "                    indiv_subcategory_correctness.append(0)\n",
    "\n",
    "        #append results of last iteration\n",
    "        sub_category_list.append(\"Total\")\n",
    "        correct_list.append(correct_analogies_subcategory)\n",
    "        total_list.append(total_analogies_subcategory)\n",
    "        skipped_list.append(skipped_subcategory)\n",
    "        indiv_correctness_list.append(indiv_subcategory_correctness)\n",
    "            \n",
    "        correct_sum = sum(correct_list)\n",
    "        correct_list.append(correct_sum)\n",
    "\n",
    "        total_sum = sum(total_list)\n",
    "        total_list.append(total_sum)\n",
    "\n",
    "        skipped_sum = sum(skipped_list)\n",
    "        skipped_list.append(skipped_sum)\n",
    "\n",
    "\n",
    "        #don't return initial 0\n",
    "        return sub_category_list,correct_list[1:],total_list[1:],skipped_list[1:],indiv_correctness_list[1:]\n",
    "    \n",
    "    def bulk_eval(self):\n",
    "        indiv_dict=dict()\n",
    "        \n",
    "        for i,d in enumerate(self.dimensions):\n",
    "            custom_glove_str = \"25customv2.\"+d+\"d\"\n",
    "            pretrain_glove_str=\"glove.6B.\"+d+\"d\"\n",
    "            \n",
    "            self.file_prefix = custom_glove_str\n",
    "            self.load_word_vectors()\n",
    "            self.freq_threshold=2\n",
    "            \n",
    "            \n",
    "            print(f\"Radiopaedia embedding {d} dimensions\")\n",
    "            \n",
    "            category_list,correct_list,total_list,skipped_list,indiv_correctness_list = self.eval_analogy()\n",
    "            dat = {'Category':[custom_glove_str +\"_\"+s for s in category_list], 'Correct':correct_list,\n",
    "                   \"Total Analogy\":total_list,\"Skipped\":skipped_list}\n",
    "            \n",
    "            #no individual list for \"total\"\n",
    "            for num,category in enumerate(category_list[:-1]):\n",
    "                indiv_dict[custom_glove_str+category]=indiv_correctness_list[num]\n",
    "            \n",
    "            if i == 0:\n",
    "                df_master = pd.DataFrame(data=dat)\n",
    "            else:\n",
    "                df = pd.DataFrame(data=dat)\n",
    "                df_master = df_master.append(df)\n",
    "\n",
    "            print('@','***'*25)\n",
    "            \n",
    "            self.freq_threshold = 0\n",
    "            \n",
    "            self.file_prefix=pretrain_glove_str\n",
    "            self.load_word_vectors()\n",
    "            \n",
    "            print(f\"Pretrained embedding {d} dimensions\")\n",
    "            \n",
    "            category_list,correct_list,total_list,skipped_list,indiv_correctness_list = self.eval_analogy()\n",
    "            dat = {'Category':[pretrain_glove_str +\"_\"+s for s in category_list], 'Correct':correct_list,\n",
    "                   \"Total Analogy\":total_list,\"Skipped\":skipped_list}\n",
    "            \n",
    "            for i,category in enumerate(category_list[:-1]):\n",
    "                indiv_dict[pretrain_glove_str+category]=indiv_correctness_list[i]\n",
    "\n",
    "            df = pd.DataFrame(data=dat)\n",
    "            df_master = df_master.append(df)\n",
    "            print('@','***'*25)\n",
    "            \n",
    "        #reset to smaller dimension afterwards\n",
    "        self.file_prefix = \"25customv2.50d\"\n",
    "        self.load_word_vectors()\n",
    "            \n",
    "        return df_master,indiv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = \"F:\\\\BDRAD MiniProject\\\\NLP\"\n",
    "dim = [\"50\",\"100\",\"200\",\"300\"]\n",
    "#path to pretrained embeddings folder\n",
    "GLOVE_PATH = os.path.join(main_path,'glove_pretrain')\n",
    "#path to analogy file format of a:b::c:d as a,b,c,d\n",
    "ANALOGY_PATH = os.path.join(main_path,\"med_analogy.csv\")\n",
    "\n",
    "analogy = AnalogyEvaluate(dim,GLOVE_PATH,ANALOGY_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df,indiv_dict = analogy.bulk_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare in format for R script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "dim =[50,100,200,300]\n",
    "categories = set()\n",
    "\n",
    "for i,key in enumerate(indiv_dict.keys()):\n",
    "    m = re.search(\"#.+\",key)\n",
    "    categories.add(m.group(0).replace(\"#\",\"\"))\n",
    "\n",
    "comparison_keys = list()    \n",
    "    \n",
    "for cat in categories:\n",
    "    for d in dim:\n",
    "        main_str = re.compile(\".+\"+str(d)+\"d#\"+cat)\n",
    "        for i,key in enumerate(indiv_dict.keys()):\n",
    "            m = main_str.match(f'{key}')\n",
    "            if m:\n",
    "                comparison_keys.append(m.group(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def correct_compare(v1,v2):\n",
    "    #v1, v2 must be lists of same length\n",
    "    #format should be 1 for correct, 0 for incorrect\n",
    "    \n",
    "    if len(v1)==len(v2):\n",
    "        \n",
    "        v1 = np.asarray(v1)\n",
    "        v2 = np.asarray(v2)\n",
    "        \n",
    "        #both correct\n",
    "        cc=0\n",
    "        #v1 incorrect v2 correct\n",
    "        ic=0\n",
    "        #v1 correct v2 incorrect\n",
    "        ci=0\n",
    "        #both incorrect\n",
    "        ii=0\n",
    "\n",
    "        #1 represents ci case, -1 represents ic case, 0 represnts either cc or ii\n",
    "        dif_vec = v1-v2\n",
    "        \n",
    "        for i,dif in enumerate(dif_vec):\n",
    "            if dif==1:\n",
    "                ci+=1\n",
    "            elif dif==-1:\n",
    "                ic+=1\n",
    "            elif dif==0:\n",
    "                if v1[i]==1:\n",
    "                    cc+=1\n",
    "                else:\n",
    "                    ii+=1              \n",
    "            else:\n",
    "                print(\"Vectors should be only 1's and 0's\")\n",
    "        return cc,ic,ci,ii\n",
    "    else:\n",
    "        print(\"v1 and v2 must be same length\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names =[\"c-c\",\"i-c\",\"c-i\",\"i-i\"]\n",
    "df_mcnemar = pd.DataFrame(columns = column_names)\n",
    "title_list=list()\n",
    "for i in range(0,len(comparison_keys),2):\n",
    "    title = str(comparison_keys[i]+\"_\"+comparison_keys[i+1])\n",
    "    title_list.append(title)\n",
    "    \n",
    "    output_list=correct_compare(indiv_dict[comparison_keys[i]],indiv_dict[comparison_keys[i+1]])\n",
    "    #convert from tuple\n",
    "    output_list = list(output_list)\n",
    "    output_list = np.asarray(output_list)\n",
    "    df_mcnemar.loc[i/2]=output_list\n",
    "\n",
    "df_mcnemar.insert(0,\"Category\",title_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
